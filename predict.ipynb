{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np , os\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import pickle\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input,Dropout\n",
    "import os\n",
    "import re\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "33\n",
      "{'what': 0, 'is': 1, 'tacotron': 2, '2': 3, 'the': 4, 'architecture': 5, 'of': 6, 'how': 7, 'does': 8, 'use': 9, 'deep': 10, 'learning': 11, 'generate': 12, 'speech': 13, 'difference': 14, 'between': 15, 'and': 16, 'are': 17, 'some': 18, 'limitations': 19, 'handle': 20, 'different': 21, 'languages': 22, 'accents': 23, 'background': 24, 'noise': 25, 'speaking': 26, 'styles': 27, 'emotions': 28, 'in': 29, 'role': 30, 'textencoder': 31, 'spectral': 32, 'encoder': 33, 'wavenet': 34, 'vocoder': 35, 'pitch': 36, 'prosody': 37, 'attention': 38, 'mechanism': 39, 'long': 40, 'sentences': 41, 'rare': 42, 'words': 43, 'or': 44, 'proper': 45, 'nouns': 46, 'melspectrogram': 47, 'diacritics': 48, 'encoderdecoder': 49, 'postnet': 50, 'synthesis': 51, 'for': 52, 'genders': 53, 'ages': 54, 'f0': 55, 'conditioning': 56, 'duration': 57, 'model': 58, 'dialects': 59, 'language': 60, 'meldecoder': 61, 'rates': 62, 'characterembedding': 63, 'convolutional': 64, 'layers': 65, 'voices': 66, 'lstm': 67, 'levels': 68, 'recording': 69, 'conditions': 70, 'residual': 71, 'connections': 72, 'microphones': 73, 'devices': 74, 'sampling': 75, 'prenet': 76, 'postprocessing': 77, 'mouth': 78, 'shapes': 79, 'batchnormalization': 80, 'relu': 81, 'activation': 82, 'music': 83, 'sound': 84, 'effects': 85, 'reverberation': 86, 'dropout': 87, 'layer': 88, 'adam': 89, 'optimizer': 90, 'movements': 91, 'head': 92, 'facial': 93, 'expressions': 94, 'body': 95, 'postures': 96, 'with': 97, 'sentence': 98, 'structures': 99, 'linear': 100, 'singing': 101, 'musical': 102, 'genres': 103, 'instruments': 104, 'multihead': 105, 'dilated': 106, 'convolution': 107, 'from': 108, 'spectrogram': 109, 'frequencies': 110, 'softmax': 111, 'sigmoid': 112, 'volumes': 113, 'speeds': 114, 'block': 115, 'drama': 116, 'comedy': 117, 'poetry': 118, 'storytelling': 119, 'news': 120, 'reporting': 121, 'gated': 122, 'sports': 123, 'commentary': 124, 'podcasting': 125, 'radio': 126, 'broadcasting': 127, 'voice': 128, 'acting': 129, 'public': 130, 'education': 131, 'audiobooks': 132, 'concatenation': 133, 'transpose': 134, 'highway': 135, 'network': 136, 'artificial': 137, 'intelligence': 138, 'chatbots': 139, 'virtual': 140, 'assistants': 141, 'robotics': 142, 'gaming': 143, 'decoder': 144, 'global': 145, 'local': 146, 'upsampling': 147, 'asmr': 148, 'meditation': 149, 'hypnosis': 150, 'relaxation': 151, 'sleep': 152, 'aid': 153, 'therapy': 154, 'timedelay': 155, 'neural': 156, 'logarithmic': 157, 'audio': 158, 'branding': 159, 'advertising': 160, 'voiceovers': 161, 'jingles': 162, 'design': 163, 'causal': 164, 'video': 165, 'games': 166, 'augmented': 167, 'reality': 168, 'positional': 169, 'encoding': 170, 'normalization': 171, 'elearning': 172, 'moocs': 173, 'instructional': 174, 'videos': 175, 'fast': 176, 'fourier': 177, 'transform': 178, 'inverse': 179, 'audiometry': 180, 'hearing': 181, 'aids': 182, 'cochlear': 183, 'implants': 184, 'telemedicine': 185, 'remote': 186, 'conferencing': 187, 'bidirectional': 188, 'teaching': 189, 'translation': 190, 'recognition': 191, 'authentication': 192, 'recurrent': 193, 'unit': 194, 'speechtotext': 195, 'conversion': 196, 'texttospeech': 197, 'coding': 198, 'beam': 199, 'search': 200, 'greedy': 201, 'decoding': 202, 'compression': 203, 'enhancement': 204, 'zeropadding': 205, 'melfrequency': 206, 'cepstral': 207, 'coefficients': 208, 'shorttime': 209, 'denoising': 210, 'convergence': 211, 'clipping': 212, 'contrast': 213, 'stretching': 214, 'subtraction': 215, 'smoothing': 216, 'widening': 217, 'multiple': 218, 'speakers': 219, 'padding': 220, 'magnitude': 221, 'overlapping': 222, 'scaling': 223, 'phase': 224, 'inversion': 225, 'envelope': 226, 'frame': 227, 'size': 228, 'shift': 229, 'resolution': 230, 'frequency': 231, 'range': 232, 'window': 233, 'text': 234, 'spectrograms': 235, 'character': 236, 'embeddings': 237, 'networks': 238, 'shortterm': 239, 'memory': 240, 'compare': 241, 'to': 242, 'other': 243, 'methods': 244, 'advantages': 245, 'can': 246, 'be': 247, 'improved': 248, 'training': 249, 'process': 250, 'it': 251, 'take': 252, 'train': 253, 'do': 254, 'you': 255, 'finetune': 256, 'specific': 257, 'data': 258, 'augmentation': 259, 'effect': 260, 'hyperparameters': 261, 'on': 262, '2s': 263, 'performance': 264, 'outofvocabulary': 265, 'punctuation': 266, 'intonation': 267, 'traditional': 268, 'systems': 269, 'perform': 270, 'nonenglish': 271, 'speaker': 272, 'sounds': 273, 'networkbased': 274, 'rate': 275, 'short': 276, 'batch': 277, 'emphasis': 278, 'loss': 279, 'functions': 280, 'optimizers': 281, 'homonyms': 282, 'homophones': 283, 'sarcasm': 284, 'irony': 285, 'spoken': 286, 'vs': 287, 'written': 288, 'slang': 289, 'informal': 290, 'types': 291, 'affective': 292, 'interruptions': 293, 'disfluencies': 294, 'hesitation': 295, 'uncertainty': 296, 'breathing': 297, 'pauses': 298, 'preprocessing': 299, 'abbreviations': 300, 'acronyms': 301, 'foreign': 302, 'phrases': 303, 'sizes': 304, 'stride': 305, 'tone': 306, 'schedules': 307, 'nonstandard': 308, 'spellings': 309, 'typos': 310, 'depths': 311, 'widths': 312, 'phonetic': 313, 'variation': 314, 'weight': 315, 'initialization': 316, 'boundaries': 317, 'contractions': 318, 'elisions': 319, 'homograph': 320, 'disambiguation': 321, 'meanings': 322, 'decay': 323, 'gradient': 324, 'homophone': 325, 'contextdependent': 326, 'distributions': 327, 'tying': 328, 'regularization': 329, 'tokenization': 330, 'named': 331, 'entities': 332, 'mechanisms': 333, 'idiomatic': 334, 'architectures': 335, 'compound': 336, 'early': 337, 'stopping': 338, 'criteria': 339, 'phrasal': 340, 'verbs': 341, 'strengths': 342, 'pooling': 343, 'ambiguous': 344, 'pronouns': 345, 'hidden': 346, 'function': 347, 'derivatives': 348, 'negation': 349, 'double': 350, 'negatives': 351, 'positions': 352, 'syntactic': 353, 'ambiguity': 354, 'factor': 355, 'sequencetosequence': 356, 'a': 357, 'uses': 358, 'an': 359, 'learn': 360, 'patterns': 361, 'generates': 362, 'by': 363, 'predicting': 364, 'melspectrograms': 365, 'using': 366, 'has': 367, 'naturalness': 368, 'compared': 369, 'original': 370, 'struggle': 371, 'multilingual': 372, 'datasets': 373, 'accentspecific': 374, 'adding': 375, 'multispeaker': 376, 'emotionspecific': 377, 'encodes': 378, 'input': 379, 'into': 380, 'fixedlength': 381, 'vector': 382, 'processes': 383, 'representation': 384, 'final': 385, 'waveform': 386, 'contour': 387, 'focuses': 388, 'relevant': 389, 'parts': 390, 'handles': 391, 'pretrained': 392, 'pronunciation': 393, 'lexicon': 394, 'represents': 395, 'acoustic': 396, 'features': 397, 'diacritictophoneme': 398, 'converter': 399, 'maps': 400, 'refines': 401, 'improves': 402, 'synthesize': 403, 'genderspecific': 404, 'agespecific': 405, 'prediction': 406, 'predicts': 407, 'phoneme': 408, 'through': 409, 'phonemetographeme': 410, 'alignment': 411, 'finetuning': 412, 'word': 413, 'sequence': 414, 'predicted': 415, 'emotional': 416, 'control': 417, 'converts': 418, 'characters': 419, 'vectors': 420, 'extract': 421, 'highlevel': 422, 'speakerspecific': 423, 'temporal': 424, 'dependencies': 425, 'states': 426, 'highquality': 427, 'noiseaware': 428, 'enable': 429, 'improve': 430, 'microphonespecific': 431, 'devicespecific': 432, 'resampling': 433, 'retraining': 434, 'extracts': 435, 'lowlevel': 436, 'quality': 437, 'reduces': 438, 'artifacts': 439, 'lipsync': 440, 'normalizes': 441, 'output': 442, 'applies': 443, 'nonlinearity': 444, 'musicaware': 445, 'embedding': 446, 'injection': 447, 'prevents': 448, 'overfitting': 449, 'randomly': 450, 'dropping': 451, 'units': 452, 'during': 453, 'updates': 454, 'weights': 455, 'minimize': 456, 'attends': 457, 'signal': 458, 'increases': 459, 'receptive': 460, 'field': 461, 'larger': 462, 'context': 463, 'inputs': 464, 'probability': 465, 'distribution': 466, '0': 467, '1': 468, 'maintains': 469, 'information': 470, 'depth': 471, 'adapts': 472, 'linguistic': 473, 'controls': 474, 'flow': 475, 'wavenets': 476, 'convolutions': 477, 'combines': 478, 'textual': 479, 'upsamples': 480, 'enables': 481, 'meaningful': 482, 'learned': 483, 'latent': 484, 'representations': 485, 'level': 486, 'microphone': 487, 'characteristics': 488, 'device': 489, 'nonacoustic': 490, 'better': 491, 'style': 492, 'helps': 493, 'longterm': 494, 'modeling': 495, 'lowprobability': 496, 'events': 497, 'voiceover': 498, 'jingle': 499, 'ensures': 500, 'that': 501, 'only': 502, 'past': 503, 'used': 504, 'refine': 505, 'generated': 506, 'waveforms': 507, 'adds': 508, 'position': 509, 'helping': 510, 'capture': 511, 'order': 512, 'activations': 513, 'within': 514, 'each': 515, 'improving': 516, 'stability': 517, 'accelerating': 518, 'educational': 519, 'massive': 520, 'open': 521, 'online': 522, 'courses': 523, 'mooc': 524, 'content': 525, 'domain': 526, 'back': 527, 'timedomain': 528, 'adapting': 529, 'audiological': 530, 'contexts': 531, 'therapeutic': 532, 'scenarios': 533, 'customizing': 534, 'optimizing': 535, 'implant': 536, 'processors': 537, 'medical': 538, 'teleconsultations': 539, 'conference': 540, 'both': 541, 'forward': 542, 'backward': 543, 'directions': 544, 'capturing': 545, 'sides': 546, 'instruction': 547, 'related': 548, 'tasks': 549, 'applications': 550, 'variant': 551, 'selectively': 552, 'transcription': 553, 'purposes': 554, 'diverse': 555, 'requirements': 556, 'algorithm': 557, 'explores': 558, 'paths': 559, 'enhancing': 560, 'strategy': 561, 'selects': 562, 'most': 563, 'probable': 564, 'at': 565, 'step': 566, 'maintain': 567, 'preserves': 568, 'timefrequency': 569, 'processing': 570, 'scales': 571, 'stabilize': 572, 'measures': 573, 'similarity': 574, 'synthesized': 575, 'ground': 576, 'truth': 577, 'truncates': 578, 'certain': 579, 'emphasizes': 580, 'peaks': 581, 'resizes': 582, 'fixed': 583, 'length': 584, 'estimates': 585, 'removes': 586, 'highfrequency': 587, 'expands': 588, 'zeros': 589, 'match': 590, 'desired': 591, 'captures': 592, 'amplitude': 593, 'adjusts': 594, 'scale': 595, 'impacting': 596, 'perceived': 597, 'loudness': 598, 'affecting': 599, 'timing': 600, 'harmonics': 601, 'component': 602, 'flips': 603, 'spectrum': 604, 'modifying': 605, 'timbre': 606, 'describes': 607, 'shape': 608, 'influencing': 609, 'determines': 610, 'framewindow': 611, 'compute': 612, 'defines': 613, 'overlap': 614, 'consecutive': 615, 'frameswindows': 616, 'computation': 617, 'detail': 618, 'sets': 619, 'covered': 620, 'reducing': 621, 'leakage': 622, 'learningbased': 623, 'system': 624, 'combination': 625, 'encode': 626, 'previous': 627, 'convert': 628, 'continuous': 629, 'signals': 630, 'produces': 631, 'more': 632, 'naturalsounding': 633, 'than': 634, 'may': 635, 'unusual': 636, 'natural': 637, 'incorporating': 638, 'version': 639, 'focus': 640, 'large': 641, 'dataset': 642, 'paired': 643, 'examples': 644, 'several': 645, 'days': 646, 'even': 647, 'weeks': 648, 'depending': 649, 'hardware': 650, 'additional': 651, 'techniques': 652, 'such': 653, 'as': 654, 'changing': 655, 'impact': 656, 'models': 657, 'time': 658, 'fallback': 659, 'subword': 660, 'add': 661, 'advanced': 662, 'performs': 663, 'well': 664, 'not': 665, 'robust': 666, 'stateoftheart': 667, 'affects': 668, 'speed': 669, 'accuracy': 670, 'but': 671, 'complex': 672, 'produce': 673, 'unnatural': 674, 'usage': 675, 'affect': 676, 'designed': 677, 'prepares': 678, 'pronounce': 679, 'them': 680, 'correctly': 681, 'their': 682, 'windows': 683, 'potentially': 684, 'smaller': 685, 'strides': 686, 'contours': 687, 'cyclic': 688, 'reduce': 689, 'lower': 690, 'generally': 691, 'correct': 692, 'deepr': 693, 'computationally': 694, 'expensive': 695, 'wider': 696, 'require': 697, 'computational': 698, 'resources': 699, 'based': 700, 'xavier': 701, 'pause': 702, 'beams': 703, 'l2': 704, 'norm': 705, 'square': 706, 'root': 707, 'no': 708, 'clear': 709, 'best': 710, 'method': 711, 'dynamic': 712, 'various': 713, 'along': 714, 'including': 715, 'part': 716}\n",
      "717\n",
      "717\n",
      "722\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "\n",
    "folder_path = 'data_generation/newQuestions'  # Geben Sie den Pfad zum gewünschten Ordner an\n",
    "\n",
    "# Durchlaufen aller Dateien im Ordner\n",
    "que = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path)]\n",
    "folder_path = 'data_generation/newAnswers' \n",
    "ans = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path)]\n",
    "print(len(que))\n",
    "print(len(ans))\n",
    "#my better version\n",
    "file_path = './processed_data/'\n",
    "\n",
    "with open(file_path+'all_fragen.txt', 'r') as inputfile:\n",
    "    questions += [line.rstrip() for line in inputfile.readlines()]\n",
    "    \n",
    "\n",
    "with open(file_path+'all_antworten.txt', 'r') as inputfile:\n",
    "    answers += [line.rstrip() for line in inputfile.readlines()]\n",
    "\n",
    "def make_clean(sentences):\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        txt = sentence.lower()\n",
    "        txt = re.sub(r\"i'm\", \"i am\", txt)\n",
    "        txt = re.sub(r\"he's\", \"he is\", txt)\n",
    "        txt = re.sub(r\"she's\", \"she is\", txt)\n",
    "        txt = re.sub(r\"that's\", \"that is\", txt)\n",
    "        txt = re.sub(r\"what's\", \"what is\", txt)\n",
    "        txt = re.sub(r\"where's\", \"where is\", txt)\n",
    "        txt = re.sub(r\"\\'ll\", \" will\", txt)\n",
    "        txt = re.sub(r\"\\'ve\", \" have\", txt)\n",
    "        txt = re.sub(r\"\\'re\", \" are\", txt)\n",
    "        txt = re.sub(r\"\\'d\", \" would\", txt)\n",
    "        txt = re.sub(r\"won't\", \"will not\", txt)\n",
    "        txt = re.sub(r\"can't\", \"can not\", txt)\n",
    "        txt = re.sub(r\"[^\\w\\s]\", \"\", txt)\n",
    "        result += [txt]\n",
    "    return result\n",
    "\n",
    "#print(antworten)\n",
    "clean_ques = make_clean(questions)\n",
    "clean_ans = make_clean(answers)\n",
    "###  count occurences ###\n",
    "counter_words = {}\n",
    "\n",
    "for line in clean_ques:\n",
    "    for word in line.split():\n",
    "        if word not in counter_words:\n",
    "            counter_words[word] = 1\n",
    "        else:\n",
    "            counter_words[word] += 1\n",
    "for line in clean_ans:\n",
    "    for word in line.split():\n",
    "        if word not in counter_words:\n",
    "            counter_words[word] = 1\n",
    "        else:\n",
    "            counter_words[word] += 1\n",
    "vocab = {}\n",
    "word_num = 0\n",
    "for word, count in counter_words.items():\n",
    "    vocab[word] = word_num\n",
    "    word_num += 1\n",
    "print(vocab)\n",
    "print(len(vocab))    \n",
    "\n",
    "for i in range(len(clean_ans)):\n",
    "    clean_ans[i] = '<SOS> ' + clean_ans[i] + ' <EOS>'\n",
    "\n",
    "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
    "x = len(vocab)\n",
    "print(x)\n",
    "for token in tokens:\n",
    "    vocab[token] = x\n",
    "    x += 1\n",
    "    \n",
    "    \n",
    "vocab['cameron'] = vocab['<PAD>']\n",
    "vocab['<PAD>'] = 0\n",
    "print(len(vocab))\n",
    "\n",
    "### inv answers dict ###\n",
    "inv_vocab = {w:v for v, w in vocab.items()}\n",
    "\n",
    "encoder_inp = []\n",
    "for line in clean_ques:\n",
    "    lst = []\n",
    "    for word in line.split():\n",
    "        if word not in vocab:\n",
    "            lst.append(vocab['<OUT>'])\n",
    "        else:\n",
    "            lst.append(vocab[word])\n",
    "        \n",
    "    encoder_inp.append(lst)\n",
    "\n",
    "decoder_inp = []\n",
    "for line in clean_ans:\n",
    "    lst = []\n",
    "    for word in line.split():\n",
    "        if word not in vocab:\n",
    "            lst.append(vocab['<OUT>'])\n",
    "        else:\n",
    "            lst.append(vocab[word])        \n",
    "    decoder_inp.append(lst)\n",
    "\n",
    "### delete\n",
    "del(clean_ans, clean_ques, line, lst, word)    \n",
    "\n",
    "longest_sequence = 0\n",
    "for x in encoder_inp+decoder_inp:\n",
    "    \n",
    "    if len(x) > longest_sequence:\n",
    "        longest_sequence = len(x)     \n",
    "print(longest_sequence) \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "encoder_inp = pad_sequences(encoder_inp, longest_sequence, padding='post', truncating='post')\n",
    "decoder_inp = pad_sequences(decoder_inp, longest_sequence, padding='post', truncating='post')\n",
    "\n",
    "def make_vector_to_sequence(input, dic):\n",
    "    ints = []\n",
    "    for k in input:\n",
    "        try:\n",
    "            ints.append(inv_vocab[k])\n",
    "        except Exception as e:\n",
    "            ints.append(inv_vocab[2])\n",
    "        \n",
    "    return ints    \n",
    "decoder_final_output = []\n",
    "for i in decoder_inp:\n",
    "    decoder_final_output.append(i[1:]) \n",
    "\n",
    "decoder_final_output = pad_sequences(decoder_final_output, longest_sequence, padding='post', truncating='post')\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "num_categories = len(vocab)  # Check the actual size of your vocabulary\n",
    "\n",
    "# Debugging statements\n",
    "#print(\"Number of categories:\", num_categories)\n",
    "#print(\"Decoder final output shape:\", decoder_final_output.shape)\n",
    "#print(\"Maximum value in decoder final output:\", decoder_final_output.max())\n",
    "\n",
    "# Convert to one-hot encoded representation\n",
    "decoder_final_output = to_categorical(decoder_final_output, num_categories)\n",
    "\n",
    "#print(decoder_final_output.shape)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input\n",
    "\n",
    "# for both\n",
    "\n",
    "enc_inp = Input(shape=(longest_sequence, ))\n",
    "dec_inp = Input(shape=(longest_sequence, ))\n",
    "# embedding layer reduces dimensionality\n",
    "# Compress output dim -> voc 722  bzw 300 \n",
    "VOCAB_SIZE = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enc_model(encoded_states,modelname):\n",
    "        # Load the encoder states\n",
    "    with open(\"new_saves/\"+ 'encoder_states.pkl', 'rb') as f:\n",
    "        loaded_enc_states = pickle.load(f)\n",
    "\n",
    "    # Load the model (including architecture and weights)\n",
    "    loaded_model = tf.keras.models.load_model(\"new_saves/\"+ 'model.h5')\n",
    "\n",
    "    # Define the decoder input and states input layers\n",
    "    decoder_state_input_h = Input(shape=(800,))\n",
    "    decoder_state_input_c = Input(shape=(800,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    # Get the relevant layers from the loaded model\n",
    "    dec_embed = loaded_model.get_layer('embedding_2')  # Use the correct name for the embedding layer\n",
    "    dec_lstm = loaded_model.get_layer('lstm_6') \n",
    "\n",
    "    # Create the decoder model\n",
    "    decoder_outputs, state_h, state_c = dec_lstm(dec_embed.output, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "\n",
    "    loaded_dec_model = Model([dec_lstm.input] + decoder_states_inputs,\n",
    "                            [decoder_outputs] + decoder_states)\n",
    "    return loaded_dec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model = Model([enc_inp], enc_states)\n",
    "\n",
    "# decoder Model\n",
    "decoder_state_input_h = Input(shape=(800,))\n",
    "decoder_state_input_c = Input(shape=(800,))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# wir verwenden LSTM units mit den trainierten states\n",
    "# h war hidden state mit Information Kontext \n",
    "# c cell state mit Inforamtion über Abhängigkeiten\n",
    "decoder_outputs, state_h, state_c = dec_lstm(dec_embed , \n",
    "                                    initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "dec_model = Model([dec_inp]+ decoder_states_inputs,\n",
    "                                    [decoder_outputs]+ decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_inp = Input(shape=(longest_sequence, ))\n",
    "dec_inp = Input(shape=(longest_sequence, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_enc_and_dec_model(encoded_states,modelname):\n",
    "        # Load the encoder states\n",
    "    with open(\"new_saves/\"+ 'encoder_states.pkl', 'rb') as f:\n",
    "        loaded_enc_states = pickle.load(f)\n",
    "\n",
    "    # Load the model (including architecture and weights)\n",
    "    loaded_model = tf.keras.models.load_model(\"new_saves/\"+ 'model.h5')\n",
    "    \n",
    "    enc_model = Model([enc_inp], loaded_enc_states)\n",
    "\n",
    "    # Get the relevant layers from the loaded model\n",
    "    enc_lstm = loaded_model.get_layer('lstm_6')        # Use the correct name for the encoder LSTM layer\n",
    "    dec_lstm = loaded_model.get_layer('lstm_7')        # Use the correct name for the decoder LSTM layer\n",
    "    embed = loaded_model.get_layer('embedding_2')      # Use the correct name for the embedding layer\n",
    "\n",
    "    # Create the encoder model\n",
    "    enc_states = [Input(shape=(800,)), Input(shape=(800,))]\n",
    "    enc_op, h, c = enc_lstm(embed.output, initial_state=enc_states)\n",
    "    enc_states = [h, c]\n",
    "    \n",
    "\n",
    "    # Create the decoder model\n",
    "    decoder_state_input_h = Input(shape=(800,))\n",
    "    decoder_state_input_c = Input(shape=(800,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    decoder_outputs, state_h, state_c = dec_lstm([embed.output] + decoder_states_inputs,\n",
    "                                                initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    dec_model = Model([embed.input] + decoder_states_inputs,\n",
    "                    [decoder_outputs] + decoder_states)\n",
    "    return enc_model,dec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input tensor cannot be reached given provided output tensors. Please make sure the tensor KerasTensor(type_spec=TensorSpec(shape=(None, 800), dtype=tf.float32, name='input_20'), name='input_20', description=\"created by layer 'input_20'\") is included in the model inputs when building functional model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ben\\Desktop\\Sequence_to_sequence_about_tacotron_2\\predict.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m enc_model,dec_model \u001b[39m=\u001b[39m get_enc_and_dec_model(\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Ben\\Desktop\\Sequence_to_sequence_about_tacotron_2\\predict.ipynb Cell 5\u001b[0m in \u001b[0;36mget_enc_and_dec_model\u001b[1;34m(encoded_states, modelname)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m enc_op, h, c \u001b[39m=\u001b[39m enc_lstm(embed\u001b[39m.\u001b[39moutput, initial_state\u001b[39m=\u001b[39menc_states)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m enc_states \u001b[39m=\u001b[39m [h, c]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m enc_model \u001b[39m=\u001b[39m Model([embed\u001b[39m.\u001b[39;49minput] \u001b[39m+\u001b[39;49m enc_states, enc_states)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Create the decoder model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m decoder_state_input_h \u001b[39m=\u001b[39m Input(shape\u001b[39m=\u001b[39m(\u001b[39m800\u001b[39m,))\n",
      "File \u001b[1;32mc:\\Users\\Ben\\anaconda3\\envs\\Test2\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ben\\anaconda3\\envs\\Test2\\lib\\site-packages\\keras\\engine\\functional.py:164\u001b[0m, in \u001b[0;36mFunctional.__init__\u001b[1;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mexecuting_eagerly_outside_functions():\n\u001b[0;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m    159\u001b[0m         [\n\u001b[0;32m    160\u001b[0m             functional_utils\u001b[39m.\u001b[39mis_input_keras_tensor(t)\n\u001b[0;32m    161\u001b[0m             \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(inputs)\n\u001b[0;32m    162\u001b[0m         ]\n\u001b[0;32m    163\u001b[0m     ):\n\u001b[1;32m--> 164\u001b[0m         inputs, outputs \u001b[39m=\u001b[39m functional_utils\u001b[39m.\u001b[39;49mclone_graph_nodes(\n\u001b[0;32m    165\u001b[0m             inputs, outputs\n\u001b[0;32m    166\u001b[0m         )\n\u001b[0;32m    167\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_graph_network(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\Ben\\anaconda3\\envs\\Test2\\lib\\site-packages\\keras\\engine\\functional_utils.py:155\u001b[0m, in \u001b[0;36mclone_graph_nodes\u001b[1;34m(inputs, outputs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclone_graph_nodes\u001b[39m(inputs, outputs):\n\u001b[0;32m    137\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Clone the `Node` between the inputs and output tensors.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \n\u001b[0;32m    139\u001b[0m \u001b[39m    This function is used to create a new functional model from any intermediate\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39m      to create a new functional model.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m     nodes_to_clone \u001b[39m=\u001b[39m find_nodes_by_inputs_and_outputs(inputs, outputs)\n\u001b[0;32m    156\u001b[0m     cloned_inputs \u001b[39m=\u001b[39m []\n\u001b[0;32m    157\u001b[0m     cloned_outputs \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Ben\\anaconda3\\envs\\Test2\\lib\\site-packages\\keras\\engine\\functional_utils.py:115\u001b[0m, in \u001b[0;36mfind_nodes_by_inputs_and_outputs\u001b[1;34m(inputs, outputs)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[39m# In case this is the tf.keras.Input node, we have reached the end\u001b[39;00m\n\u001b[0;32m    111\u001b[0m         \u001b[39m# of the tracing of upstream nodes. Any further tracing will just be\u001b[39;00m\n\u001b[0;32m    112\u001b[0m         \u001b[39m# an infinite loop. we should raise an error here since we didn't\u001b[39;00m\n\u001b[0;32m    113\u001b[0m         \u001b[39m# find the input in the user-specified inputs.\u001b[39;00m\n\u001b[0;32m    114\u001b[0m         \u001b[39mif\u001b[39;00m inbound_node\u001b[39m.\u001b[39mis_input:\n\u001b[1;32m--> 115\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    116\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mFound input tensor cannot be reached given provided \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39moutput tensors. Please make sure the tensor \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mincluded in the model inputs when building \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfunctional model.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(kt)\n\u001b[0;32m    120\u001b[0m             )\n\u001b[0;32m    121\u001b[0m         nodes_to_visit\u001b[39m.\u001b[39mappend(inbound_node)\n\u001b[0;32m    123\u001b[0m \u001b[39m# Do a final check and make sure we have reached all the user-specified\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# inputs\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input tensor cannot be reached given provided output tensors. Please make sure the tensor KerasTensor(type_spec=TensorSpec(shape=(None, 800), dtype=tf.float32, name='input_20'), name='input_20', description=\"created by layer 'input_20'\") is included in the model inputs when building functional model."
     ]
    }
   ],
   "source": [
    "enc_model,dec_model = get_enc_and_dec_model(\"\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 719]\n"
     ]
    }
   ],
   "source": [
    "def create_pred_input(sentence):\n",
    "    X_in = []\n",
    "    for word in sentence.lower().split():\n",
    "        try:\n",
    "            X_in.append(vocab[word])\n",
    "        except:\n",
    "            X_in.append(vocab['<OUT>'])\n",
    "            pass\n",
    "    print(X_in)    \n",
    "    return pad_sequences([X_in], longest_sequence, padding='post')\n",
    "\n",
    "input_for_prediction = create_pred_input('What is Tacotron 2 ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentences):\n",
    "    sentences = make_clean(sentences)\n",
    "    for sentence in sentences:\n",
    "        print(\"Question  : \" + sentence + \"/n\")\n",
    "        pred_input = create_pred_input(sentence)\n",
    "        stat = enc_model.predict(pred_input)\n",
    "        print(\"make predicted Input for \" + str(pred_input))\n",
    "        empty_target_seq = np.zeros( ( 1 , 1) )\n",
    "        ##   empty_target_seq = [0]\n",
    "        empty_target_seq[0, 0] = vocab['<SOS>']       \n",
    "\n",
    "        stop_condition = False\n",
    "        decoded_translation = ''\n",
    "        while not stop_condition :\n",
    "\n",
    "            dec_outputs , h, c= dec_model.predict([ empty_target_seq] + stat )\n",
    "            decoder_concat_input = dense(dec_outputs)\n",
    "            ## decoder_concat_input = [0.1, 0.2, .4, .0, ...............]\n",
    "\n",
    "            sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )\n",
    "            ## sampled_word_index = [2]\n",
    "\n",
    "            sampled_word = inv_vocab[sampled_word_index] + ' '\n",
    "\n",
    "            ## inv_vocab[2] = 'hi'\n",
    "            ## sampled_word = 'hi '\n",
    "\n",
    "            if sampled_word != '<EOS> ':\n",
    "                decoded_translation += sampled_word  \n",
    "\n",
    "            if sampled_word == '<EOS> ' or len(decoded_translation.split()) > longest_sequence:\n",
    "                stop_condition = True \n",
    "\n",
    "            empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "            empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "            ## <SOS> - > hi\n",
    "            ## hi --> <EOS>\n",
    "            stat = [h, c]  \n",
    "\n",
    "        print(\"chatbot : \", decoded_translation )\n",
    "        print(\"============================================== \\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question  : what is tacotron 2/n\n",
      "[0, 1, 2, 3]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Ben\\anaconda3\\envs\\Test2\\lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Ben\\anaconda3\\envs\\Test2\\lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Ben\\anaconda3\\envs\\Test2\\lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\Ben\\anaconda3\\envs\\Test2\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\Ben\\anaconda3\\envs\\Test2\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Ben\\anaconda3\\envs\\Test2\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_2\" expects 3 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 26) dtype=int32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ben\\Desktop\\Sequence_to_sequence_about_tacotron_2\\predict.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Testset \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mHow does Tacotron 2 utilize a Sequence-to-Sequence architecture?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mWhat is the purpose of the Spectral Encoder in Tacotron 2?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mHow does Tacotron 2 generate speech from spectrograms?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mWhat is the role of the WaveNet Vocoder in Tacotron 2\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mHow are character embeddings used in Tacotron 2?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mWhat are the advantages of using convolutional networks in Tacotron 2?\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#print(])\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m predict_sentence([questions[\u001b[39m0\u001b[39;49m],questions[\u001b[39m1\u001b[39;49m],questions[\u001b[39m2\u001b[39;49m]])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#predict_sentence(answers[0])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m Testset \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mCan the Tacotron 2 deal with speech\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mCan it generate speech\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mwhat are current challenges\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mIs it a sequence2sequ\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m            ]\n",
      "\u001b[1;32mc:\\Users\\Ben\\Desktop\\Sequence_to_sequence_about_tacotron_2\\predict.ipynb Cell 7\u001b[0m in \u001b[0;36mpredict_sentence\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mQuestion  : \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m sentence \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/n\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pred_input \u001b[39m=\u001b[39m create_pred_input(sentence)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m stat \u001b[39m=\u001b[39m enc_model\u001b[39m.\u001b[39;49mpredict(pred_input)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmake predicted Input for \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(pred_input))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ben/Desktop/Sequence_to_sequence_about_tacotron_2/predict.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m empty_target_seq \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros( ( \u001b[39m1\u001b[39m , \u001b[39m1\u001b[39m) )\n",
      "File \u001b[1;32mc:\\Users\\Ben\\anaconda3\\envs\\Test2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file45mpw94l.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Ben\\anaconda3\\envs\\Test2\\lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Ben\\anaconda3\\envs\\Test2\\lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Ben\\anaconda3\\envs\\Test2\\lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\Ben\\anaconda3\\envs\\Test2\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\Ben\\anaconda3\\envs\\Test2\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Ben\\anaconda3\\envs\\Test2\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_2\" expects 3 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 26) dtype=int32>]\n"
     ]
    }
   ],
   "source": [
    "Testset = [\"How does Tacotron 2 utilize a Sequence-to-Sequence architecture?\",\n",
    "\"What is the purpose of the Spectral Encoder in Tacotron 2?\",\n",
    "\"How does Tacotron 2 generate speech from spectrograms?\"\n",
    "\"What is the role of the WaveNet Vocoder in Tacotron 2\",\n",
    "\"How are character embeddings used in Tacotron 2?\",\n",
    "\"What are the advantages of using convolutional networks in Tacotron 2?\"]\n",
    "#print(])\n",
    "predict_sentence([questions[0],questions[1],questions[2]])\n",
    "#predict_sentence(answers[0])\n",
    "Testset = [\"Can the Tacotron 2 deal with speech\",\n",
    "           \"Can it generate speech\",\n",
    "           \"what are current challenges\",\n",
    "           \"Is it a sequence2sequ\"\n",
    "           ]\n",
    "predict_sentence(Testset)\n",
    "Testset = [\"Is Tacotron an apple ?\"]\n",
    "predict_sentence(Testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
