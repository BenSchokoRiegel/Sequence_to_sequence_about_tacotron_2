{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t1():\n",
    "    VOCAB_SIZE = len(vocab)\n",
    "embed = Embedding(VOCAB_SIZE+1, output_dim=300, \n",
    "                  input_length=longest_sequence,\n",
    "                  trainable=True                  \n",
    "                  )\n",
    "\n",
    "\n",
    "enc_embed = embed(enc_inp)\n",
    "enc_lstm = LSTM(600, return_sequences=True, return_state=True)\n",
    "enc_op, h, c = enc_lstm(enc_embed)\n",
    "enc_states = [h, c]\n",
    "\n",
    "\n",
    "\n",
    "dec_embed = embed(dec_inp)\n",
    "dec_lstm = LSTM(600, return_sequences=True, return_state=True, dropout=0.)\n",
    "dec_op, _, _ = dec_lstm(dec_embed, initial_state=enc_states)\n",
    "\n",
    "dense = Dense(VOCAB_SIZE, activation='softmax')\n",
    "\n",
    "dense_op = dense(dec_op)\n",
    "\n",
    "model = Model([enc_inp, dec_inp], dense_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalModel():\n",
    "    # embedding layer reduces dimensionality\n",
    "    # Compress output dim -> voc 722  bzw 300 \n",
    "    embed = Embedding(VOCAB_SIZE+1, output_dim=300, \n",
    "                    input_length=longest_sequence,\n",
    "                    trainable=True                  \n",
    "                    )\n",
    "    enc_embed = embed(enc_inp)\n",
    "    # erstelen LSTM mit dimensionilität von 400 \n",
    "    # sequence von eingabeVektoren in Ausgabe Vektoren\n",
    "    # h (hidden state): Der letzte verborgene Zustand des LSTM-Layers. \n",
    "    # Der verborgene Zustand enthält Kontext der Eingabesequenz bis zur letzten Zeitstufe.\n",
    "    # c (cell state): Der letzte Zellzustand des LSTM-Layers. \n",
    "    # Der Zellzustand speichert Informationen über  langfristige Abhängigkeiten in Eingabesequenz.\n",
    "    enc_lstm = LSTM(800, return_sequences=True, return_state=True)\n",
    "    enc_op, h, c = enc_lstm(enc_embed)\n",
    "    enc_states = [h, c]\n",
    "    # Decoder LSTM\n",
    "    dec_embed = embed(dec_inp)\n",
    "    dec_lstm = LSTM(800, return_sequences=True, return_state=True)\n",
    "    dec_op, _, _ = dec_lstm(dec_embed, initial_state=enc_states)\n",
    "\n",
    "    # output is propability of how propable the words are\n",
    "    dense = Dense(VOCAB_SIZE, activation='softmax')\n",
    "    # dense verknüpfen mit decoder output\n",
    "    dense_op = dense(dec_op)\n",
    "    return Model([enc_inp, dec_inp], dense_op),dec_embed,enc_states,dec_lstm,dense"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
